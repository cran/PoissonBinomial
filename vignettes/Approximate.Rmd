---
title: "Approximate Procedures"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Approximate Procedures}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this package, the following approximation algorithms for computing the Poisson Binomial distribution with Bernoulli probabilities $p_1, ..., p_n$ are implemented:

* the *Poisson Approximation* approach,
* the *Arithmetic Mean Binomial Approximation* procedure,
* *Geometric Mean Binomial Approximation* algorithms and
* *Normal* and *Refined Normal Approximation*s.

The computation of these procedures is optimized and accelerated by some simple preliminary considerations:

1. Are all $p_i$ equal?    
In This case, we have an ordinary binomial distribution. The specified method of computation is then ignored.
2. Are all of the $p_i (i = 1, ..., n)$ 0 or 1?    
If one $p_i$ is 1, it is impossible to measure 0 successes. Following the same logic, if two $p_i$ are 1, we cannot observe 0 and 1 successes and so on. In general, a number of $n_1 > 0$ values $p_i = 1$ makes it impossible to measure $0, ..., n_1 - 1$ successes. Likewise, if $n_0 > 0$ of the $p_i = 0$, we cannot observe $n - n_0 + 1, ..., n$ successes. This leads to three cases (the specified method of computation is ignored in any of them):
    a) All $p_i = 0$: The only observable value is $0$, i.e. $P(X = 0) = 1$ and $P(X \neq 0) = 0$.
    b) All $p_i = 1$: The only observable value is $n$, i.e. $P(X = n) = 1$ and $P(X \neq n) = 0$.
    c) All $p_i \in \{0, 1\}$: The only observable value is $n_1$, i.e. $P(X = n_1) = 1$ and $P(X \neq n_1) = 0$.
3. Are there $p_i \notin \{0, 1\}$?    
Then the only observable values are $n_1, n_1 + 1, ..., n - n_0$, i.e. $P(X \in \{n_1, ..., n - n_0\}) > 0$ and $P(X < n_1) = P(X > n - n_0) = 0$. As a result, $X$ can be expressed as $X = n_1 + Y$ with $Y \sim PBin(\{p_i|0 < p_i < 1\})$ and $|\{p_i|0 < p_i < 1\}| = n - n_0 - n_1$. Thus, the Poisson Binomial distribution must only be computed for $Y$.

These cases are illustrated in the following example:

```{r ex}
library(PoissonBinomial)

# Case 1
dpbinom(NULL, rep(0.3, 7))
dbinom(0:7, 7, 0.3)
# equal results

# Case 2
dpbinom(NULL, c(0, 0, 0, 0, 0, 0, 0))
dpbinom(NULL, c(1, 1, 1, 1, 1, 1, 1))
dpbinom(NULL, c(0, 0, 0, 0, 1, 1, 1))

# Case 3
dpbinom(NULL, c(0, 0, 0.4, 0.2, 0.8, 0.1, 1), method = "RefinedNormal")
```


## Poisson Approximation

The *Poisson Approximation* (DC) approach is requested with `method = "Poisson"`. It is based on a Poisson distribution, whose parameter is the sum of the probabilities of success.

```{r pa1}
set.seed(1)
pp <- runif(10)
wt <- sample(1:10, 10, TRUE)

dpbinom(NULL, pp, wt, "Poisson")
ppbinom(NULL, pp, wt, "Poisson")
```

A comparison with exact computation shows that the approximation quality of the PA procedure increases with smaller probabilities of success. The reason is that the Poisson Binomial distribution approaches a Poisson distribution when the probabilities are very small.

```{r pa2}
set.seed(1)

# U(0, 1) random probabilities of success
pp <- runif(20)
ppbinom(NULL, pp, method = "Poisson")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "Poisson") - ppbinom(NULL, pp))

# U(0, 0.01) random probabilities of success
pp <- runif(20, 0, 0.01)
ppbinom(NULL, pp, method = "Poisson")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "Poisson") - ppbinom(NULL, pp))
```


## Arithmetic Mean Binomial Approximation

The *Arithmetic Mean Binomial Approximation* (AMBA) approach is requested with `method = "Mean"`. It is based on a Binomial distribution, whose parameter is the arithmetic mean of the probabilities of success.

```{r am1}
set.seed(1)
pp <- runif(10)
wt <- sample(1:10, 10, TRUE)
mean(rep(pp, wt))

dpbinom(NULL, pp, wt, "Mean")
ppbinom(NULL, pp, wt, "Mean")
```

A comparison with exact computation shows that the approximation quality of the AMBA procedure increases when the probabilities of success are closer to each other. The reason is that, although the expectation remains unchanged, the distribution's variance becomes smaller the less the probabilities differ. Since this variance is minimized by equal probabilities (but still underestimated), the AMBA method is best suited for situations with very similar probabilities of success.

```{r am2}
set.seed(1)

# U(0, 1) random probabilities of success
pp <- runif(20)
ppbinom(NULL, pp, method = "Mean")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "Mean") - ppbinom(NULL, pp))

# U(0.4, 0.6) random probabilities of success
pp <- runif(20, 0.3, 0.5)
ppbinom(NULL, pp, method = "Mean")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "Mean") - ppbinom(NULL, pp))

# U(0.49, 0.51) random probabilities of success
pp <- runif(20, 0.39, 0.41)
ppbinom(NULL, pp, method = "Mean")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "Mean") - ppbinom(NULL, pp))
```


## Geometric Mean Binomial Approximation - Variant A

The *Geometric Mean Binomial Approximation (Variant A)* (GMBA-A) approach is requested with `method = "GeoMean"`. It is based on a Binomial distribution, whose parameter is the geometric mean of the probabilities of success:
$$\hat{p} = \sqrt[n]{p_1 \cdot ... \cdot p_n}$$

```{r gma1}
set.seed(1)
pp <- runif(10)
wt <- sample(1:10, 10, TRUE)
prod(rep(pp, wt))^(1/sum(wt))

dpbinom(NULL, pp, wt, "GeoMean")
ppbinom(NULL, pp, wt, "GeoMean")
```

It is known that the geometric mean of the probabilities of success is always smaller than their arithmetic mean. Thus, we get a stochastically *smaller* binomial distribution. A comparison with exact computation shows that the approximation quality of the GMBA-A procedure increases when the probabilities of success are closer to each other:

```{r gma2}
set.seed(1)

# U(0, 1) random probabilities of success
pp <- runif(20)
ppbinom(NULL, pp, method = "GeoMean")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "GeoMean") - ppbinom(NULL, pp))

# U(0.4, 0.6) random probabilities of success
pp <- runif(20, 0.4, 0.6)
ppbinom(NULL, pp, method = "GeoMean")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "GeoMean") - ppbinom(NULL, pp))

# U(0.49, 0.51) random probabilities of success
pp <- runif(20, 0.49, 0.51)
ppbinom(NULL, pp, method = "GeoMean")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "GeoMean") - ppbinom(NULL, pp))
```


## Geometric Mean Binomial Approximation - Variant B

The *Geometric Mean Binomial Approximation (Variant B)* (GMBA-B) approach is requested with `method = "GeoMeanCounter"`. It is based on a Binomial distribution, whose parameter is 1 minus the geometric mean of the probabilities of **failure**:
$$\hat{p} = 1 - \sqrt[n]{(1 - p_1) \cdot ... \cdot (1 - p_n)}$$

```{r gmb1}
set.seed(1)
pp <- runif(10)
wt <- sample(1:10, 10, TRUE)
1 - prod(1 - rep(pp, wt))^(1/sum(wt))

dpbinom(NULL, pp, wt, "GeoMeanCounter")
ppbinom(NULL, pp, wt, "GeoMeanCounter")
```

It is known that the geometric mean of the probabilities of success is always greater than their arithmetic mean. Thus, we get a stochastically *larger* binomial distribution. A comparison with exact computation shows that the approximation quality of the GMBA-B procedure again increases when the probabilities of success are closer to each other:

```{r gmb2}
set.seed(1)

# U(0, 1) random probabilities of success
pp <- runif(20)
ppbinom(NULL, pp, method = "GeoMeanCounter")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "GeoMeanCounter") - ppbinom(NULL, pp))

# U(0.4, 0.6) random probabilities of success
pp <- runif(20, 0.4, 0.6)
ppbinom(NULL, pp, method = "GeoMeanCounter")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "GeoMeanCounter") - ppbinom(NULL, pp))

# U(0.49, 0.51) random probabilities of success
pp <- runif(20, 0.49, 0.51)
ppbinom(NULL, pp, method = "GeoMeanCounter")
ppbinom(NULL, pp)
summary(ppbinom(NULL, pp, method = "GeoMeanCounter") - ppbinom(NULL, pp))
```


## Normal Approximation

The *Normal Approximation* (NA) approach is requested with `method = "Normal"`. It is based on a Normal distribution, whose parameters are derived from the theoretical mean and variance of the input probabilities of success.

```{r na1}
set.seed(1)
pp <- runif(10)
wt <- sample(1:10, 10, TRUE)
mean(rep(pp, wt))

dpbinom(NULL, pp, wt, "Normal")
ppbinom(NULL, pp, wt, "Normal")
```

A comparison with exact computation shows that the approximation quality of the NA procedure increases with larger numbers of probabilities of success:

```{r na2}
set.seed(1)

# U(0, 1) random probabilities of success
pp <- runif(10)
summary(ppbinom(NULL, pp, method = "Normal") - ppbinom(NULL, pp))

# U(0.4, 0.6) random probabilities of success
pp <- runif(1000)
summary(ppbinom(NULL, pp, method = "Normal") - ppbinom(NULL, pp))

# U(0.49, 0.51) random probabilities of success
pp <- runif(100000)
summary(ppbinom(NULL, pp, method = "Normal") - ppbinom(NULL, pp))
```


## Refined Normal Approximation

The *Refined Normal Approximation* (RNA) approach is requested with `method = "RefinedNormal"`. It is based on a Normal distribution, whose parameters are derived from the theoretical mean, variance and skewness of the input probabilities of success.

```{r rna1}
set.seed(1)
pp <- runif(10)
wt <- sample(1:10, 10, TRUE)
mean(rep(pp, wt))

dpbinom(NULL, pp, wt, "RefinedNormal")
ppbinom(NULL, pp, wt, "RefinedNormal")
```

A comparison with exact computation shows that the approximation quality of the RNA procedure increases with larger numbers of probabilities of success:

```{r rna2}
set.seed(1)

# U(0, 1) random probabilities of success
pp <- runif(10)
summary(ppbinom(NULL, pp, method = "RefinedNormal") - ppbinom(NULL, pp))

# U(0.4, 0.6) random probabilities of success
pp <- runif(1000)
summary(ppbinom(NULL, pp, method = "RefinedNormal") - ppbinom(NULL, pp))

# U(0.49, 0.51) random probabilities of success
pp <- runif(100000)
summary(ppbinom(NULL, pp, method = "RefinedNormal") - ppbinom(NULL, pp))
```


## Performance Comparisons

To assess the performance of the approximation procedures, we use the `microbenchmark` package. Each algorithm has to calculate the PMF repeatedly based on random probability vectors. The run times are then summarized in a table that presents, among other statistics, their minima, maxima and means. The following results were recorded on an AMD Ryzen 7 1800X with 32 GiB of RAM and Ubuntu 18.04.3 (running inside a VirtualBox VM; the host system is Windows 10 Education).

```{r benchmark}
library(microbenchmark)
set.seed(1)

f1 <- function() dpbinom(NULL, runif(4000), method = "Normal")
f2 <- function() dpbinom(NULL, runif(4000), method = "RefinedNormal")
f3 <- function() dpbinom(NULL, runif(4000), method = "Poisson")
f4 <- function() dpbinom(NULL, runif(4000), method = "Mean")
f5 <- function() dpbinom(NULL, runif(4000), method = "GeoMean")
f6 <- function() dpbinom(NULL, runif(4000), method = "GeoMeanCounter")
f7 <- function() dpbinom(NULL, runif(4000), method = "DivideFFT")

microbenchmark(f1(), f2(), f3(), f4(), f5(), f6(), f7())
```

Clearly, the NA procedure is the fastest, followed by the RNA method, which needs roughly 30-40% more time, and the PA, AMBA and GMBA approaches that need almost twice as long as the NA algorithm. AMBA, GMBA-A and GMBA-B procedures exhibit almost equal mean execution speed, with the AMBA algorithm being slightly faster. All of the approximation procedures outperform the fastest exact approach, DC-FFT, by far. Even the slowest approximate algorithm is around 4x as fast as DC-FFT.